{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405976c1-c0f7-4b98-bec3-d13dadbc15a4",
   "metadata": {},
   "source": [
    "# Let's Parse our Mummer Alignments! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28cc4e24-c4f7-4503-aa98-4abf85c16ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some tools\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "from Bio import SeqIO\n",
    "from collections import defaultdict\n",
    "# CLI stuff for MP\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b666893-e23a-4d4d-a3df-07e399cd1e92",
   "metadata": {},
   "source": [
    "## Define our main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d1600-76a2-411c-bca1-46b151f180d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_genbank(path):\n",
    "    records = defaultdict()\n",
    "    for record in SeqIO.parse(path, 'genbank'):\n",
    "        records[record.id] = record\n",
    "    return records\n",
    "\n",
    "def get_alignment_files(alignments_dir):\n",
    "    alignments = glob.glob(f'{alignments_dir}/**/align_coords.tsv', recursive=True)\n",
    "    return alignments\n",
    "\n",
    "def check_alignment(path):\n",
    "    # Let's make sure there's actually alignments within this file. Many such cases of no alignment. (Expected)\n",
    "    # {{TO-DO: Add way to gather no-alns into a table}}\n",
    "    with open(path, 'r') as infile:\n",
    "        lines = infile.readlines()\n",
    "    if len(lines) == 1:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def parse_alignment(path):\n",
    "    # Read the file and parse the table.\n",
    "    with open(path, 'r') as infile:\n",
    "        lines = infile.readlines()\n",
    "    keys = lines[0].strip().split('\\t')\n",
    "    for line in lines[1::]:\n",
    "        values = line.strip().split('\\t')\n",
    "        alignments.append({key:value for key,value in zip(keys, values)}) #zippity split\n",
    "    return alignments\n",
    "\n",
    "def is_within_range(feature, start, end):\n",
    "    #print(feature)\n",
    "    start_check = int(feature.location.start) >= start\n",
    "    end_check = int(feature.location.end) <= end\n",
    "    checked = (start_check and end_check) # gotta be within the range! {{TO-DO: Implement partial gene hit identification}}\n",
    "    return checked\n",
    "\n",
    "def get_features_from_range(record, start, end):\n",
    "    # gonna pull the whole feature\n",
    "    features = [feature for feature in record.features if is_within_range(feature, start, end) and feature.type == 'CDS']\n",
    "    return features\n",
    "\n",
    "def simplify_genes_for_contig(features):\n",
    "    genes = []\n",
    "    for feature in features:\n",
    "        locus_tag = feature.qualifiers['locus_tag'][0].strip(\"'\").strip('[').strip(']') # thanks python.\n",
    "        product = ' '.join(feature.qualifiers['product'])\n",
    "        gene = (locus_tag, product)\n",
    "        genes.append(gene)\n",
    "    return genes\n",
    "\n",
    "def get_coverage(length, coords):\n",
    "    covered_positions = set()\n",
    "    for position in coords:\n",
    "        start = position[0]\n",
    "        end = position[1]\n",
    "        if start > end:  # Note, this should only be required for calculation of coverage, feature extraction shouldn't need it.\n",
    "            start, end = end, start\n",
    "        covered_positions.update(range(start, end+1))\n",
    "    percent_coverage = (len(covered_positions)/length)*100\n",
    "    return percent_coverage\n",
    "\n",
    "def get_homologies(alignments, ref_dict, asm_dict):\n",
    "    # get_genes_from_alignments() => get_homologies()\n",
    "    genes_dict = defaultdict() # set up dict for alignments\n",
    "    for alignment in alignments: # iterate through each alignment for this particular comparison.\n",
    "        # for whatever reason, mummer indicates the reference as the query and the assembly as the ref. Whatever. just be mindful.\n",
    "        ref_id            = alignment['QUERY_ID']\n",
    "        ref_name          = alignment['QUERY_NAME']\n",
    "        ref_start         = int(alignment['QUERY_START'])\n",
    "        ref_end           = int(alignment['QUERY_END'])\n",
    "        ref_aln_length    = int(alignment['QUERY_LENGTH']) # ALIGNED LENGTH)\n",
    "        ref_length        = len(ref_dict[ref_name].seq)\n",
    "        assembly_id       = alignment['REF_ID']\n",
    "        contig_name       = alignment['REF_NAME']#.replace(\"0000\", \"_\").replace(\"_0\", \"_\")\n",
    "        contig_start      = int(alignment['REF_START'])\n",
    "        contig_end        = int(alignment['REF_END'])\n",
    "        contig_aln_length = int(alignment['REF_LENGTH']) # ALIGNED LENGTH)\n",
    "        aln_identity      = alignment['IDENTITY']\n",
    "        contig_length     = len(asm_dict[contig_name].seq)\n",
    "        ref_features      = get_features_from_range(ref_dict[ref_name], ref_start, ref_end)\n",
    "        asm_features      = get_features_from_range(asm_dict[contig_name], contig_start, contig_end)\n",
    "        ref_genes         = simplify_genes_for_contig(ref_features)\n",
    "        asm_genes         = simplify_genes_for_contig(asm_features)\n",
    "        #asm_genes = \"placeholder :)\"\n",
    "        # Get percent coverage for each alignment\n",
    "        # UPDATE DON'T DO THAT HERE THE RANGES GET ALL WEIRD.\n",
    "        #ref_coverage = get_coverage(ref_length, ref_start, ref_end)\n",
    "        #contig_coverage = get_coverage(contig_length, contig_start, contig_end)\n",
    "\n",
    "        if contig_name not in genes_dict: # if the contig is not already in the dict, add it, also add contig_len to its own key.\n",
    "            genes_dict[contig_name] = defaultdict(dict)\n",
    "            genes_dict[contig_name]['contig_length'] = int(contig_length) # Honestly for simplicity I should prob just make a separate dict for this.\n",
    "\n",
    "        if ref_name not in genes_dict[contig_name]: # if the ref is not in the contig dict, add it and set val to an empty list.\n",
    "            genes_dict[contig_name][ref_name] = []\n",
    "\n",
    "        alignment_dict = {\n",
    "                        'ref_aln': {\n",
    "                                'ref_length': int(ref_length),\n",
    "                                'start': int(ref_start),\n",
    "                                'end': int(ref_end),\n",
    "                                'aln_length': ref_aln_length,\n",
    "                                'percent_cov': int(),\n",
    "                                'features': ref_genes,\n",
    "                            },\n",
    "                        'asm_aln': {\n",
    "                                'start': int(contig_start),\n",
    "                                'end': int(contig_end),\n",
    "                                'aln_length': int(contig_aln_length),\n",
    "                                'aln_identity': aln_identity, # to the reference, this will probably get confusing downstream :)\n",
    "                                'percent_cov': int(),\n",
    "                                'features': asm_genes,\n",
    "                            },\n",
    "                    } # def the dictionary to append to the particular ref alignment for this single contig.\n",
    "\n",
    "        genes_dict[contig_name][ref_name].append(alignment_dict)\n",
    "    return genes_dict\n",
    "\n",
    "def make_table_for_asm(genes_dict, output_path):\n",
    "    lines = []\n",
    "    header_row = 'contig_id\\tcontig_len\\tref\\tref_len\\tcontig_cov\\treference_cov\\tgenes_on_ref\\tgenes_on_contig\\n'\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        lines.append(header_row)\n",
    "        for contig in genes_dict:\n",
    "            ref_genes = []\n",
    "            contig_genes = []\n",
    "            for aln in genes_dict[contig].keys():\n",
    "                ref_region_genes = []\n",
    "                asm_region_genes = []\n",
    "                if aln != 'contig_length': # see above to-do re: separate dict just for len/assembly stats.\n",
    "                    contig_len = genes_dict[contig]['contig_length']\n",
    "                    aln_coords = [(cov['asm_aln']['start'],cov['asm_aln']['end']) for cov in genes_dict[contig][aln]]\n",
    "                    ref_aln_coords = [(cov['ref_aln']['start'],cov['ref_aln']['end']) for cov in genes_dict[contig][aln]]\n",
    "                    ref_len = genes_dict[contig][aln][0]['ref_aln']['ref_length']\n",
    "                    for index, item in enumerate(genes_dict[contig][aln]):\n",
    "                        ref_region_genes = [gene[0] for gene in item['ref_aln']['features']]\n",
    "                        ref_genes.extend(ref_region_genes) \n",
    "                        contig_region_genes = [gene[0] for gene in item['asm_aln']['features']]\n",
    "                        contig_genes.extend(contig_region_genes)\n",
    "                    contig_cov = get_coverage(contig_len, aln_coords)\n",
    "                    #print(contig_cov, aln_coords)\n",
    "                    reference_cov = get_coverage(ref_len, ref_aln_coords)\n",
    "                    output_row = f'{contig}\\t{contig_len}\\t{aln}\\t{ref_len}\\t{contig_cov:.2f}\\t{reference_cov:.2f}\\t{len(ref_genes)}\\t{len(contig_genes)}\\n'\n",
    "                    lines.append(output_row)\n",
    "        outfile.writelines(lines)\n",
    "\n",
    "def get_rows(assembly_id, alignments, genes_dict):\n",
    "    \"\"\" Take the alns and format rows for dumping to a table separate from the main output\"\"\"\n",
    "    rows = []\n",
    "    header = ['assembly_id','contig','contig_len','total_contig_coverage','list_of_alignments(ref:contig_cov:location)']\n",
    "    rows.append(header)\n",
    "    for contig in genes_dict:\n",
    "        contig_list = []\n",
    "        contig_len = genes_dict[contig]['contig_length']\n",
    "        total_contig_cov = 0\n",
    "        for aln in genes_dict[contig].keys():\n",
    "            if aln != 'contig_length':\n",
    "                asm_coords = [(cov['asm_aln']['start'],cov['asm_aln']['end']) for cov in genes_dict[contig][aln]]\n",
    "                ref_coords = [(cov['ref_aln']['start'],cov['ref_aln']['end']) for cov in genes_dict[contig][aln]]\n",
    "                ref_len = genes_dict[contig][aln][0]['ref_aln']['ref_length']\n",
    "                contig_cov = get_coverage(contig_len, asm_coords)\n",
    "                #collapsed_coords = collapse_coords(asm_coords)\n",
    "                #asm_location = check_location_of_alignment(contig_len, asm_coords)\n",
    "                #print(check_location_of_alignment(contig_len, collapse_coords(asm_coords)))\n",
    "                ref_cov = get_coverage(ref_len, ref_coords)\n",
    "                ref_aln = (f'{aln}: {contig_cov:.2f}%')\n",
    "                contig_list.append((ref_aln))\n",
    "                total_contig_cov += contig_cov\n",
    "        row = [assembly_id, contig,contig_len,f'{total_contig_cov:.2f}']\n",
    "        row.extend(contig_list)\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def write_rows(rows, output_file):\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "        writer.writerows(rows)\n",
    "\n",
    "def parse_b31_alns(alignment, annotations_dir, ref_dict, output_dir):\n",
    "    sample_id = alignment.split('/')[-2]\n",
    "    #print(f'parsing alignments for {sample_id}!')\n",
    "    assembly = f'{annotations_dir}/{sample_id}/{sample_id}.gbff'\n",
    "    #print(f'gbff for {sample_id} exists: {os.path.exists(assembly)}')\n",
    "    #print(assembly)\n",
    "    asm_dict = parse_genbank(assembly)\n",
    "    alignments = parse_alignment(alignment)\n",
    "    homologies = get_homologies(alignments, ref_dict, asm_dict)\n",
    "    output_file_detailed = f'{output_dir}/detailed_coverage/{sample_id}_B31_Synteny.tsv'\n",
    "    make_table_for_asm(homologies, output_file_detailed')\n",
    "    rows = get_rows(sample_id, alignments, homologies)\n",
    "    output_file_simple = f'{output_dir}/simple_coverage/{sample_id}_coverage.tsv'\n",
    "    write_rows(rows, output_file_simple)\n",
    "\n",
    "def run_all_b31_alns(alignments_dir, annotations_dir, reference_genbank, output_dir):\n",
    "    ref_dict = parse_genbank(reference_genbank)\n",
    "    alignment_files = glob.glob(f'{alignments_dir}/*/align_coords.tsv')\n",
    "    for file in alignment_files:\n",
    "        parse_b31_alns(alignment, annotations_dir, ref_dict, output_dir)\n",
    "        #print(f'Finished! moving on')\n",
    "\n",
    "def parse_ids_from_filename(alignment):\n",
    "    alignment_dir_name = os.path.dirname(alignment).split('/')[-1]\n",
    "    asm1_id = os.path.basename(alignment_dir_name).split('_vs_')[0]\n",
    "    asm2_id = os.path.basename(alignment_dir_name).split('_vs_')[1]\n",
    "    return asm1_id, asm2_id\n",
    "\n",
    "def parse_single_pair_aln(alignment, annotations_dir):\n",
    "    # {{TO-DO: Not at this point but at some point I need to pull in the plasmid ID to name mapping dict.}}\n",
    "    \n",
    "    ## okay let's get our ids.\n",
    "    asm1_id, asm2_id = parse_ids_from_filename(alignment)\n",
    "    \n",
    "    ## First let's check to see that there are actually alignments for this pair.\n",
    "    completion_msg = f'Parsed homology between {asm1_id} and {asm2_id}!'\n",
    "    if not check_alignment(alignment):\n",
    "        completion_msg = f'NO HOMOLOGY BETWEEN {asm1_id} and {asm2_id}!'\n",
    "        return (False, completion_msg, [asm1_id, asm2_id]) # return the two ids if no homology!\n",
    "        \n",
    "    ### ok, it's not empty, let's parse this out.\n",
    "    alignments = parse_alignment(alignment)\n",
    "    \n",
    "    ## now let's parse the genbanks\n",
    "    asm1_gb = f'{annotations_dir}/{asm1_id}.gbff'\n",
    "    asm2_gb = f'{annotations_dir}/{asm2_id}.gbff'\n",
    "    asm1_dict = parse_genbank(asm1_gb)\n",
    "    asm2_dict = parse_genbank(asm2_gb)\n",
    "    \n",
    "    # Ok so we aren't dumping an individual file for each of these, we're taking the rows and catting them into \n",
    "    # a big list of rows for a single table to be able to elucidate all of the homologies for each plasmid.\n",
    "\n",
    "    ## Anyway let's get our genes for these alignments and actually *parse* the alignments.\n",
    "    homologies = get_homologies(alignments, asm1_dict, asm2_dict)\n",
    "\n",
    "    ## Now let's make our rows.\n",
    "    rows = get_rows(asm1_id, alignments, homologies)\n",
    "    completion_msg = f'Parsed homology between {asm1_id} and {asm2_id}!'\n",
    "    return (True, completion_msg, rows)\n",
    "\n",
    "def parallel_parse(cpus, alignment_files, annotations_dir):\n",
    "    # def parse_all_v_all():\n",
    "    # this may require parallelization?\n",
    "    # yeah let's just go ahead and do that.\n",
    "    all_rows = []\n",
    "    no_homology = []\n",
    "    with ProcessPoolExecutor(max_workers=cpus) as executor: # need to specify this elsewhere.\n",
    "        futures = []\n",
    "        # okay so we need to get all of our individual alignments, build a list of args, then feed em into the workers.\n",
    "        # first let's parse the alignments and figure out how to divide this.\n",
    "        # Wait, it's literally just iteration through a list.\n",
    "        # anyway let's set up the big list o' rows.\n",
    "        for alignment in alignment_files:\n",
    "            futures.append(executor.submit(parse_single_pair_aln, alignment, annotations_dir)) # feed our single command the aln and the dir for the genomes\n",
    "        # ok now let's gather each alignment and cat it onto the list of rows!\n",
    "        with tqdm(total=len(futures)) as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result[0] is True:\n",
    "                        all_rows.append(result[2])\n",
    "                    else:\n",
    "                        no_homology.append(result[2])\n",
    "                except Exception as e:\n",
    "                    tqdm.write(f\"Error: {e}\")\n",
    "                    tqdm.write(f\"Error: {e}\")# double it so we can keep a record on the screen.\n",
    "                pbar.update(1)\n",
    "                custom_write(result[1])\n",
    "    return all_rows, no_homology\n",
    "\n",
    "def custom_write(text):\n",
    "    # Use the tqdm.write method to ensure that the progress bar does not get disrupted\n",
    "    tqdm.write(text)\n",
    "    # Move the cursor up one line and clear the line\n",
    "    sys.stdout.write('\\033[F\\033[K')\n",
    "\n",
    "def main():\n",
    "    # Create the parser\n",
    "    parser = argparse.ArgumentParser(description=\"A script to run mauve on a directory of assemblies against the B31 reference genome.\")\n",
    "    # Add the arguments\n",
    "    parser.add_argument('alignments_dir',  type=str, help='The directory containing the alignments to parse')\n",
    "    parser.add_argument('annotations_dir', type=str, help='The directory containing the annotations (genbanks)')\n",
    "    parser.add_argument('output_dir',      type=str, help='The directory for outputs')\n",
    "    parser.add_argument('cpus',            type=int, help='How many cores we rippin')\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.mkdir(args.annotations_dir)\n",
    "    \n",
    "    alignment_files = get_alignment_files(args.alignments_dir)\n",
    "    rows, no_homologies = parallel_parse(args.cpus, alignment_files, args.annotations_dir)\n",
    "    output_alns = f'{output_dir}/ava_homo.tsv'\n",
    "    output_no_aln = f'{output_dir}/ava_no_homo.tsv'\n",
    "    write_rows(rows, output_alns)\n",
    "    write_rows(no_homologies, output_no_aln)\n",
    "    print('Finished!')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50f100-2f36-4546-9368-12f21364079f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Put excluded/bad functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9113d8-6b00-4ced-8e3f-f49b7b67be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a scuffed attempt and should not be implemented. \n",
    "#def get_position(length, start, end):\n",
    "#    # okay so where are we on the contig?\n",
    "#    if start == 0 and end <= length / 2:\n",
    "#        position = \"LE\"\n",
    "#    elif start <= 200 and end <= length/2:\n",
    "#        position = \"LS\"\n",
    "#    elif start >= 200 and end <= length/2:\n",
    "#        position = \"LS\"\n",
    "#    elif start >= length / 2 and end == length:\n",
    "#        position = \"RE\"\n",
    "#    elif start >= length / 2 and end >= length - 200:\n",
    "#        position = \"RS\"\n",
    "#    elif start >= length / 2 and end <= length - 200:\n",
    "#        position = \"RS\"\n",
    "#    elif start <= length / 2 and end >= length / 2:\n",
    "#        position = \"MID\"\n",
    "#    elif start == 0 and end == length:\n",
    "#        position = \"ENTIRE\"\n",
    "#    elif 200 >= start >= 0 and length-200 >= end >= length:\n",
    "#        position = \"NEARLY\"\n",
    "#    else:\n",
    "#        position = f\"out of bounds??{start}{end}\"\n",
    "#        print(start, end, \" !!!!! \", length)\n",
    "#    return position\n",
    "\n",
    "#def check_location_of_alignment(length, coords):\n",
    "#    # coordinate shenanigans ofc\n",
    "#    positions = []\n",
    "#    if len(coords) > 1:\n",
    "#        gap_flag = '_*_'\n",
    "#        for start, end in coords:\n",
    "#            if start > end:\n",
    "#                start, end = end, start\n",
    "#                rev = '*'\n",
    "#            else:\n",
    "#                rev = ''\n",
    "#            position = f'{rev}{get_position(length, start, end)}{rev}'\n",
    "#            positions.append(position)\n",
    "#        location = f'{gap_flag}'.join(positions)\n",
    "#    else:\n",
    "#        for start, end in coords:\n",
    "#            if start > end:\n",
    "#                start, end = end, start\n",
    "#                rev = '*'\n",
    "#            else:\n",
    "#                rev = ''\n",
    "#            position = get_position(length, start, end)\n",
    "#            location = f'{rev}{position}{rev}'\n",
    "#    return location\n",
    "\n",
    "#def collapse_coords(coords):\n",
    "#    # I do not think this is really required...\n",
    "#    coords.sort(key=lambda x: x[0])\n",
    "#    collapsed_ranges = []\n",
    "#    cur_start, cur_end = coords[0]\n",
    "#    for start, end in coords[1:]:\n",
    "#        if start <= cur_end + 1:\n",
    "#            cur_end = max(cur_end, end)\n",
    "#        else:\n",
    "#            collapsed_ranges.append((cur_start, cur_end))\n",
    "#            cur_start, cur_end = start, end\n",
    "#\n",
    "#    collapsed_ranges.append((cur_start, cur_end))\n",
    "#    return collapsed_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d8db41-e34e-4193-b263-8d032fafea84",
   "metadata": {},
   "source": [
    "## Let's parse our alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "db6a99b1-7d8f-4cf4-8ffe-139550d839e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing alignments for URI87H!\n",
      "parsing alignments for URI34H!\n",
      "parsing alignments for URI88H!\n",
      "parsing alignments for URI33H!\n",
      "parsing alignments for UCT110H!\n",
      "parsing alignments for URI39H!\n",
      "parsing alignments for URI91H!\n",
      "parsing alignments for UCT35H!\n",
      "parsing alignments for UWI247P!\n",
      "parsing alignments for URI120H!\n",
      "parsing alignments for URI107H!\n",
      "parsing alignments for UWI263P!\n",
      "parsing alignments for URI89H!\n",
      "parsing alignments for URI42H!\n",
      "parsing alignments for URI44H!\n",
      "parsing alignments for UCT109H!\n",
      "parsing alignments for URI40H!\n",
      "parsing alignments for URI117H!\n",
      "parsing alignments for URI47H!\n",
      "parsing alignments for URI86H!\n",
      "parsing alignments for URI36H!\n",
      "parsing alignments for UNY208P!\n",
      "parsing alignments for ESI26H!\n",
      "parsing alignments for UCT31H!\n",
      "parsing alignments for URI56H!\n",
      "parsing alignments for UCT30H!\n",
      "parsing alignments for URI103H!\n",
      "parsing alignments for UCT29H!\n",
      "parsing alignments for URI112H!\n",
      "parsing alignments for UWI248P!\n",
      "parsing alignments for UNY203P!\n",
      "parsing alignments for UCT96H!\n",
      "parsing alignments for UCT32H!\n",
      "parsing alignments for UNY193P!\n",
      "parsing alignments for UCT113H!\n",
      "parsing alignments for URI93H!\n",
      "parsing alignments for UNY169P!\n",
      "parsing alignments for UWI283P!\n",
      "parsing alignments for URI102H!\n",
      "parsing alignments for URI41H!\n",
      "parsing alignments for UNY172P!\n",
      "parsing alignments for UNY149P!\n",
      "parsing alignments for UCT92H!\n",
      "parsing alignments for URI118H!\n",
      "parsing alignments for UCT50H!\n",
      "parsing alignments for URI101H!\n",
      "parsing alignments for URI46H!\n",
      "parsing alignments for URI48H!\n",
      "parsing alignments for URI111H!\n"
     ]
    }
   ],
   "source": [
    "b31_alignments_dir = '/home/mf019/longread_pangenome/synteny/pgv_mummer_output_2'\n",
    "annotations_dir = '/home/mf019/longread_pangenome/longread_analysis/paired_assemblies/paired_only/longread/annotation'\n",
    "reference_genbank = '/home/mf019/longread_pangenome/synteny/renamed_GCF_000008685.2.gbff'\n",
    "output_dir = '/home/mf019/longread_pangenome/synteny/parsing_output'\n",
    "#run_all(b31_alignments_dir, annotations_dir, reference_genbank, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99ab1c-f98f-434f-abb5-577745aa4e80",
   "metadata": {},
   "source": [
    "## Let's parse the big all v all alignments I did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e60f7dc4-5ea9-40a6-b2f3-0650d6bf1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a13ceff3-87ea-4ab9-bc9f-76591ef612c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contig</th>\n",
       "      <th>contig_len</th>\n",
       "      <th>total_contig_coverage</th>\n",
       "      <th>list_of_alignments(ref:contig_cov:location)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CP031405.1</td>\n",
       "      <td>19997</td>\n",
       "      <td>78.63</td>\n",
       "      <td>CP002320.1: 78.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CP019851.1</td>\n",
       "      <td>16820</td>\n",
       "      <td>4.10</td>\n",
       "      <td>CP074057.1: 4.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CP001458.1</td>\n",
       "      <td>38893</td>\n",
       "      <td>4.40</td>\n",
       "      <td>CP002315.1: 4.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CP001458.1</td>\n",
       "      <td>38893</td>\n",
       "      <td>3.15</td>\n",
       "      <td>CP017212.1: 3.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CP094610.1</td>\n",
       "      <td>29936</td>\n",
       "      <td>1.57</td>\n",
       "      <td>CP002314.1: 1.57%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       contig  contig_len  total_contig_coverage  \\\n",
       "0  CP031405.1       19997                  78.63   \n",
       "1  CP019851.1       16820                   4.10   \n",
       "2  CP001458.1       38893                   4.40   \n",
       "3  CP001458.1       38893                   3.15   \n",
       "4  CP094610.1       29936                   1.57   \n",
       "\n",
       "  list_of_alignments(ref:contig_cov:location)  \n",
       "0                          CP002320.1: 78.63%  \n",
       "1                           CP074057.1: 4.10%  \n",
       "2                           CP002315.1: 4.40%  \n",
       "3                           CP017212.1: 3.15%  \n",
       "4                           CP002314.1: 1.57%  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ava_homo_simple_file = 'ava_wp_db/ava_homo_simple.tsv'\n",
    "parsing_pkl = '/home/mf019/borrelia_plasmid_classifier_v3/parsing_tables/blast_parsing_dict.pkl'\n",
    "with open(parsing_pkl, 'rb') as in_file:\n",
    "    parsing_dict = pickle.load(in_file)\n",
    "ava_homo = pandas.read_csv(ava_homo_simple_file, delimiter='\\t')\n",
    "ava_homo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d54d5523-979b-4533-bcf6-719d98089530",
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_sorted = ava_homo.sort_values(by='contig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7414d59-9725-453c-8fcc-51a141064fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contig</th>\n",
       "      <th>contig_len</th>\n",
       "      <th>total_contig_coverage</th>\n",
       "      <th>list_of_alignments(ref:contig_cov:location)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15104</th>\n",
       "      <td>AE000783.1</td>\n",
       "      <td>910724</td>\n",
       "      <td>99.99</td>\n",
       "      <td>CP124092.1: 99.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>AE000783.1</td>\n",
       "      <td>910724</td>\n",
       "      <td>0.03</td>\n",
       "      <td>CP002325.1: 0.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>AE000783.1</td>\n",
       "      <td>910724</td>\n",
       "      <td>0.68</td>\n",
       "      <td>CP002306.1: 0.68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13194</th>\n",
       "      <td>AE000783.1</td>\n",
       "      <td>910724</td>\n",
       "      <td>0.68</td>\n",
       "      <td>CP001273.1: 0.68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4443</th>\n",
       "      <td>AE000783.1</td>\n",
       "      <td>910724</td>\n",
       "      <td>99.05</td>\n",
       "      <td>CP002228.1: 99.05%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           contig  contig_len  total_contig_coverage  \\\n",
       "15104  AE000783.1      910724                  99.99   \n",
       "4362   AE000783.1      910724                   0.03   \n",
       "8653   AE000783.1      910724                   0.68   \n",
       "13194  AE000783.1      910724                   0.68   \n",
       "4443   AE000783.1      910724                  99.05   \n",
       "\n",
       "      list_of_alignments(ref:contig_cov:location)  \n",
       "15104                          CP124092.1: 99.99%  \n",
       "4362                            CP002325.1: 0.03%  \n",
       "8653                            CP002306.1: 0.68%  \n",
       "13194                           CP001273.1: 0.68%  \n",
       "4443                           CP002228.1: 99.05%  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ava_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f10c94f-0e83-4505-8440-43a17a8f63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_sorted['contig_name'] = ava_sorted['contig'].apply(lambda x: parsing_dict[x]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "319a9f8d-c1f9-4a73-9b49-f48ff5ef7258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15104     chromosome :  99.99%\n",
      "4362             lp38 :  0.03%\n",
      "8653           lp28-1 :  0.68%\n",
      "13194          lp28-1 :  0.68%\n",
      "4443      chromosome :  99.05%\n",
      "                 ...          \n",
      "7854      chromosome :  99.98%\n",
      "4816      chromosome :  99.98%\n",
      "11806    chromosome :  100.03%\n",
      "2223      chromosome :  99.96%\n",
      "3550          lp28-11 :  0.11%\n",
      "Name: aln_name, Length: 15209, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ava_sorted['aln_name'] = ava_sorted['list_of_alignments(ref:contig_cov:location)'].apply(lambda x: f'{parsing_dict[x.split(':')[0]]['name']} : {x.split(':')[1]}')\n",
    "print(ava_sorted['aln_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c20a4ae-266c-42b4-b7df-b9724e69099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = ava_sorted.groupby('contig')['aln_name'].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9693ca2-137a-4d28-a45c-3d0c62ba2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table['contig_name'] = pivot_table['contig'].apply(lambda x: parsing_dict[x]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6afcf32b-27d0-4ae5-9c97-97b9ae72d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = ['contig', 'contig_name', 'aln_name']\n",
    "pivot_table = pivot_table[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc7a13da-5a3b-4d83-b9ac-f7d2eacd8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table.to_csv('ava_wp_db/ava_homo_simple_merged.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad569f29-f151-4784-998b-8e510de5a176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
